{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4X4-oRRf_Xd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "YZRyA-qEYWB2",
    "outputId": "30141124-1b3c-4668-a902-eb813d9b28ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend: tensorflow\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "#import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers, initializers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "print(\"backend:\", K.backend())\n",
    "tf_session = K.get_session()\n",
    "#from utils import plot_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPI2Sh3oj9O2"
   },
   "outputs": [],
   "source": [
    "def squash(vector, axis=-1):\n",
    "  squared_norm = K.sum(K.square(vector), axis, keepdims=True)\n",
    "  scale = squared_norm / (1 + squared_norm) / K.sqrt(squared_norm + K.epsilon())\n",
    "  return vector * scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9L77mKQgQEx"
   },
   "outputs": [],
   "source": [
    "def PrimaryCapsules(inputs, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid'):\n",
    "  output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, \n",
    "                       padding=padding, name='primarycap_conv2d')(inputs)\n",
    "  outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "  return layers.Lambda(squash, name='primarycap_squash')(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOTjwhy05IDW"
   },
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "  def __init__(self, num_capsule, dim_capsule, routings=3, kernel_initializer='glorot_uniform', **kwargs):\n",
    "    super(CapsuleLayer, self).__init__(**kwargs)\n",
    "    self.num_capsule = num_capsule\n",
    "    self.dim_capsule = dim_capsule\n",
    "    self.routings = routings\n",
    "    self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "    self.input_num_capsule = input_shape[1]\n",
    "    self.input_dim_capsule = input_shape[2]\n",
    "    \n",
    "    self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule, self.dim_capsule, self.input_dim_capsule], \n",
    "                            initializer=self.kernel_initializer, name='W')\n",
    "    \n",
    "    print(\"w shape: \", K.int_shape(self.W)) # (10, 1152, 16, 8)\n",
    "    \n",
    "    #self.W_init = tf.random_normal(shape=(1, 1152, 10, 16, 8),\n",
    "    #                         stddev=0.01, dtype=tf.float32, name=\"W_init\")\n",
    "    #self.W = tf.Variable(self.W_init, name=\"W\")\n",
    "    #self.W_tiled = tf.tile(self.W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "    #print(\"W_tiled shape: \", self.W_tiled.shape)\n",
    "    \n",
    "    self.built = True\n",
    "    \n",
    "  def call(self, inputs, training=None):\n",
    "    \n",
    "    print(\"inputs shape: \", K.int_shape(inputs)) # (None, 1152, 8)\n",
    "    \n",
    "    inputs_expand = K.expand_dims(inputs, 1)\n",
    "    #inputs_expand = layers.Lambda(lambda x: K.expand_dims(x, 1))(inputs)\n",
    "    print(\"inputs_expand shape: \", K.int_shape(inputs_expand)) # (None, 1, 1152, 8)\n",
    "    \n",
    "    inputs_tiled =  K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "    #inputs_tiled = layers.Lambda(lambda x: K.tile(x, [1, self.num_capsule, 1, 1]))(inputs_expand)\n",
    "    print(\"inputs_tiled shape: \", K.int_shape(inputs_tiled)) # (None, 10, 1152, 8)\n",
    "    \n",
    "    inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "    print(\"inputs_hat shape: \", K.int_shape(inputs_hat)) # (None, 10, None, 1152, 16)\n",
    "    \n",
    "    #inputs_expand = tf.expand_dims(inputs, -1, name=\"inputs_expand\")\n",
    "    #inputs_tile = tf.expand_dims(inputs_expand, 2, name=\"inputs_tile\")\n",
    "    #inputs_tiled = tf.tile(inputs_tile, [1, 1, 10, 1, 1], name=\"inputs_tiled\")\n",
    "    #print(\"inputs_tiled shape: \", inputs_tiled.shape)\n",
    "    #inputs_hat = tf.matmul(self.W_tiled, inputs_tiled, name=\"inputs_hat\") #prediction\n",
    "    #print(\"inputs_hat shape: \", inputs_hat.shape)\n",
    "    \n",
    "    \n",
    "    # Start: Routing algorithm\n",
    "    b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "    print(\"b (weights) shape: \", K.int_shape(b)) # (None, 10, 1152)\n",
    "    #raw_weights = tf.zeros([batch_size, 1152, 10, 1, 1], dtype=np.float32, name=\"raw_weights\")\n",
    "    \n",
    "    assert self.routings > 0, 'Routings should be > 0'\n",
    "    for i in range(self.routings):\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "            #c = tf.nn.softmax(raw_weights, dim=2, name=\"c\")\n",
    "\n",
    "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule] <-- nicht erfÃ¼llt!\n",
    "            # The first two dimensions as `batch` dimension,\n",
    "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "            \n",
    "            #weighted_predictions = tf.multiply(c, inputs_hat, name=\"weighted_predictions\")\n",
    "            #weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True, name=\"weighted_sum\")\n",
    "            #outputs = squash(weighted_sum)\n",
    "            #print(\"outputs shape: \", outputs.shape)\n",
    "            #outputs_tiled = tf.tile(outputs, [1, 1152, 1, 1, 1], name=\"outputs_tiled\")\n",
    "            #print(\"outputs_tiled shape: \", outputs_tiled)\n",
    "\n",
    "            if i < self.routings - 1:\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                \n",
    "                print(\"outputs shape: \", K.int_shape(outputs)) # (None, 10, 10, 1152, 16)\n",
    "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "                #raw_weights += tf.matmul(inputs_hat, outputs_tiled, transpose_a=True)\n",
    "                \n",
    "      # End: Routing algorithm\n",
    "\n",
    "    return outputs\n",
    "  \n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'num_capsule': self.num_capsule,\n",
    "        'dim_capsule': self.dim_capsule,\n",
    "        'routings': self.routings\n",
    "    }\n",
    "    base_config = super(CapsuleLayer, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "  \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UheC3AQh4sb"
   },
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "  def call(self, inputs, **kwargs):\n",
    "      return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "      return input_shape[:-1]\n",
    "\n",
    "  def get_config(self):\n",
    "      config = super(Length, self).get_config()\n",
    "      return config  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOAd0CwriIpo"
   },
   "outputs": [],
   "source": [
    "class Mask(layers.Layer):\n",
    "  def call(self, inputs, **kwargs):\n",
    "    if type(inputs) is list: # label is provided with shape\n",
    "      assert len(inputs) == 2\n",
    "      inputs, mask = inputs\n",
    "    else:\n",
    "      x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "      mask = K.one_hot(indices=K.argmax(x, 1), num_classes = x.get_shape().as_list()[1])\n",
    "      \n",
    "    masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "    return masked\n",
    "  \n",
    "  def compute_output_shape(self, input_shape):\n",
    "    if type(input_shape[0]) is tuple: #with class label\n",
    "      return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "    else:\n",
    "      return tuple([None, input_shape[1] * input_shape[2]])\n",
    "    \n",
    "  def get_config(self):\n",
    "    config = super(Mask, self).get_config()\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxpVlxtrk2gK"
   },
   "outputs": [],
   "source": [
    "def CapsNet(input_shape, n_class, routings):\n",
    "  x = layers.Input(shape=input_shape)\n",
    "  \n",
    "  \n",
    "  # Start: Encoder ---------------------------\n",
    "  #Layer 1: Conv2D\n",
    "  conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "  \n",
    "  #Layer 2: Conv2D with squash activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "  primary_caps = PrimaryCapsules(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "  \n",
    "  #Layer 3: Capsule Layer, Routing here\n",
    "  digit_caps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings, name='digitcaps')(primary_caps)\n",
    "  \n",
    "  # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "  # If using tensorflow, this will not be necessary. \n",
    "  out_caps = Length(name='capsnet')(digit_caps)\n",
    "  # End: Encoder ---------------------------\n",
    "  \n",
    "  \n",
    "  #Start: Decoder --------------------------\n",
    "  y = layers.Input(shape=(n_class,))\n",
    "  masked_by_y = Mask()([digit_caps, y]) # The true label is used to mask the output of capsule layer. For training\n",
    "  masked = Mask()(digit_caps) # Mask using the capsule with maximal length. For prediction\n",
    "  \n",
    "  decoder = models.Sequential(name='decoder')\n",
    "  decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
    "  decoder.add(layers.Dense(1024, activation='relu'))\n",
    "  decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
    "  decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
    "  \n",
    "  #End: Decoder ----------------------------\n",
    "  \n",
    "  train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
    "  eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
    "  \n",
    "  #manipulate model\n",
    "  \n",
    "  #noise = layers.Input(shape=(n_class, 16))\n",
    "  #noised_digitcaps = layers.Add()([digit_caps, noise])\n",
    "  #masked_noised_y = Mask()([noised_digitcaps, y])\n",
    "  #manipulate_model = models.Model([x,y,noise], decoder(masked_noised_y))\n",
    "  return train_model, eval_model#, manipulate_model\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAQncp2Hq3JL"
   },
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "  L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + 0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.9))\n",
    "  return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rYLsH5ry0i5"
   },
   "outputs": [],
   "source": [
    "def sum_squared_error(y_true, y_pred):\n",
    "    if not K.is_tensor(y_pred):\n",
    "        y_pred = K.constant(y_pred)\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "    return K.square(y_pred - y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F59Mw8vSrl39"
   },
   "outputs": [],
   "source": [
    "def train(model, data, save_dir):\n",
    "  (x_train, y_train), (x_test, y_test) = data\n",
    "  \n",
    "  log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
    "  tb = callbacks.TensorBoard(log_dir = save_dir + '/tensorboard-logs', batch_size = batch_size, histogram_freq = int(debug))\n",
    "  checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5', monitor = 'val_capsnet_acc', save_best_only = True, save_weights_only = True, verbose = 1)\n",
    "  #lr_decay = callbacks.LearningRateScheduler(schedule = lambda epoch : args.lr * (args.lr_decay ** epoch))\n",
    "  \n",
    "  model.compile(optimizer = optimizers.Adam(), loss = [margin_loss, sum_squared_error], loss_weights = [1., 0.0005], metrics = {'capsnet': 'accuracy'})\n",
    "  \n",
    "  model.fit([x_train, y_train], [y_train, x_train], batch_size = batch_size, epochs = epochs, validation_data = [[x_test, y_test], [y_test, x_test]], callbacks = [log, tb, checkpoint])\n",
    "  \n",
    "  model.fit([x_train, y_train], [y_train, x_train], batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint])\n",
    "\n",
    "  model.save_weights(save_dir + '/trained_model.h5')\n",
    "  print('Trained model saved to \\ %s/trained_model.h5\\'' %  (save_dir))\n",
    "  #plot_log(save_dir + '/log.csv', show = True)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "NX1z1af_7YWY",
    "outputId": "09d0daf4-c4d3-4869-aed5-b62ebfccb2b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\\n    image = img * 255\\n    Image.fromarray(image.astype(np.uint8)).save(save_dir + \"/real_and_recon.png\")\\n    print()\\n    print(\\'Reconstructed images are saved to %s/real_and_recon.png\\' % save_dir)\\n    print(\\'-\\' * 30 + \\'End: test\\' + \\'-\\' * 30)\\n    plt.imshow(plt.imread(save_dir + \"/real_and_recon.png\"))\\n    plt.show()'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(model, data, save_dir):\n",
    "    x_test, y_test = data\n",
    "    y_pred, x_recon = model.predict(x_test, batch_size=100)\n",
    "    print('-'*30 + 'Begin: test' + '-'*30)\n",
    "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "'''\n",
    "    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
    "    image = img * 255\n",
    "    Image.fromarray(image.astype(np.uint8)).save(save_dir + \"/real_and_recon.png\")\n",
    "    print()\n",
    "    print('Reconstructed images are saved to %s/real_and_recon.png' % save_dir)\n",
    "    print('-' * 30 + 'End: test' + '-' * 30)\n",
    "    plt.imshow(plt.imread(save_dir + \"/real_and_recon.png\"))\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW36v3UJ3D67"
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    from keras.datasets import mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    y_train = to_categorical(y_train.astype('float32'))\n",
    "    y_test = to_categorical(y_test.astype('float32'))\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "jMUtPjBM1AdY",
    "outputId": "8035acfb-dc6d-4c06-d09d-b29c42e34234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w shape:  (10, 1152, 16, 8)\n",
      "inputs shape:  (None, 1152, 8)\n",
      "inputs_expand shape:  (None, 1, None, 8)\n",
      "inputs_tiled shape:  (None, 10, None, 8)\n",
      "inputs_hat shape:  (None, 10, 1152, 16)\n",
      "b (weights) shape:  (None, 10, 1152)\n",
      "WARNING:tensorflow:From <ipython-input-4-4cfc5b1fe9a8>:57: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "outputs shape:  (None, 10, 16)\n",
      "outputs shape:  (None, 10, 16)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 256)  20992       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)      (None, 6, 6, 256)    5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 1152, 8)      0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 1152, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 10, 16)       1474560     primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 160)          0           digitcaps[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (Length)                (None, 10)           0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 28, 28, 1)    1411344     mask_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,215,568\n",
      "Trainable params: 8,215,568\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 0.5390 - capsnet_loss: 0.5389 - decoder_loss: 0.1242 - capsnet_acc: 0.1335 - val_loss: 3.4353e-05 - val_capsnet_loss: 7.9837e-07 - val_decoder_loss: 0.0671 - val_capsnet_acc: 0.3940\n",
      "\n",
      "Epoch 00001: val_capsnet_acc improved from -inf to 0.39400, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 3.2982e-05 - capsnet_loss: 1.6601e-07 - decoder_loss: 0.0656 - capsnet_acc: 0.5129 - val_loss: 3.2114e-05 - val_capsnet_loss: 6.2493e-08 - val_decoder_loss: 0.0641 - val_capsnet_acc: 0.5878\n",
      "\n",
      "Epoch 00002: val_capsnet_acc improved from 0.39400 to 0.58780, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 3.0230e-05 - capsnet_loss: 5.0201e-08 - decoder_loss: 0.0604 - capsnet_acc: 0.5862 - val_loss: 2.8454e-05 - val_capsnet_loss: 2.9987e-08 - val_decoder_loss: 0.0568 - val_capsnet_acc: 0.6281\n",
      "\n",
      "Epoch 00003: val_capsnet_acc improved from 0.58780 to 0.62810, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.7846e-05 - capsnet_loss: 3.4029e-08 - decoder_loss: 0.0556 - capsnet_acc: 0.6043 - val_loss: 2.7224e-05 - val_capsnet_loss: 2.0312e-08 - val_decoder_loss: 0.0544 - val_capsnet_acc: 0.6441\n",
      "\n",
      "Epoch 00004: val_capsnet_acc improved from 0.62810 to 0.64410, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 3.2451e-05 - capsnet_loss: 5.2838e-06 - decoder_loss: 0.0543 - capsnet_acc: 0.5270 - val_loss: 2.7030e-05 - val_capsnet_loss: 1.8161e-07 - val_decoder_loss: 0.0537 - val_capsnet_acc: 0.6000\n",
      "\n",
      "Epoch 00005: val_capsnet_acc did not improve from 0.64410\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 2.7029e-05 - capsnet_loss: 6.4782e-08 - decoder_loss: 0.0539 - capsnet_acc: 0.6582 - val_loss: 2.6753e-05 - val_capsnet_loss: 3.5378e-08 - val_decoder_loss: 0.0534 - val_capsnet_acc: 0.7274\n",
      "\n",
      "Epoch 00006: val_capsnet_acc improved from 0.64410 to 0.72740, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 3.0051e-05 - capsnet_loss: 3.1681e-06 - decoder_loss: 0.0538 - capsnet_acc: 0.5916 - val_loss: 2.6731e-05 - val_capsnet_loss: 7.2929e-08 - val_decoder_loss: 0.0533 - val_capsnet_acc: 0.6976\n",
      "\n",
      "Epoch 00007: val_capsnet_acc did not improve from 0.72740\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.6887e-05 - capsnet_loss: 5.8097e-08 - decoder_loss: 0.0537 - capsnet_acc: 0.6843 - val_loss: 2.6656e-05 - val_capsnet_loss: 5.0659e-08 - val_decoder_loss: 0.0532 - val_capsnet_acc: 0.6652\n",
      "\n",
      "Epoch 00008: val_capsnet_acc did not improve from 0.72740\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 3.9727e-05 - capsnet_loss: 1.2906e-05 - decoder_loss: 0.0536 - capsnet_acc: 0.5779 - val_loss: 2.7078e-05 - val_capsnet_loss: 4.2774e-07 - val_decoder_loss: 0.0533 - val_capsnet_acc: 0.7391\n",
      "\n",
      "Epoch 00009: val_capsnet_acc improved from 0.72740 to 0.73910, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.7092e-05 - capsnet_loss: 3.0632e-07 - decoder_loss: 0.0536 - capsnet_acc: 0.7178 - val_loss: 2.6697e-05 - val_capsnet_loss: 1.4014e-07 - val_decoder_loss: 0.0531 - val_capsnet_acc: 0.7443\n",
      "\n",
      "Epoch 00010: val_capsnet_acc improved from 0.73910 to 0.74430, saving model to ./result/weights-10.h5\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.6909e-05 - capsnet_loss: 1.4617e-07 - decoder_loss: 0.0535 - capsnet_acc: 0.7158 - val_loss: 2.6657e-05 - val_capsnet_loss: 1.0353e-07 - val_decoder_loss: 0.0531 - val_capsnet_acc: 0.7536\n",
      "\n",
      "Epoch 00011: val_capsnet_acc improved from 0.74430 to 0.75360, saving model to ./result/weights-11.h5\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 3.3577e-05 - capsnet_loss: 6.8236e-06 - decoder_loss: 0.0535 - capsnet_acc: 0.6439 - val_loss: 2.6856e-05 - val_capsnet_loss: 3.1393e-07 - val_decoder_loss: 0.0531 - val_capsnet_acc: 0.7165\n",
      "\n",
      "Epoch 00012: val_capsnet_acc did not improve from 0.75360\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 3.1338e-05 - capsnet_loss: 4.5973e-06 - decoder_loss: 0.0535 - capsnet_acc: 0.6326 - val_loss: 3.4876e-05 - val_capsnet_loss: 8.2889e-06 - val_decoder_loss: 0.0532 - val_capsnet_acc: 0.6930\n",
      "\n",
      "Epoch 00013: val_capsnet_acc did not improve from 0.75360\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.9557e-05 - capsnet_loss: 2.8481e-06 - decoder_loss: 0.0534 - capsnet_acc: 0.6969 - val_loss: 2.6723e-05 - val_capsnet_loss: 1.9666e-07 - val_decoder_loss: 0.0531 - val_capsnet_acc: 0.7684\n",
      "\n",
      "Epoch 00014: val_capsnet_acc improved from 0.75360 to 0.76840, saving model to ./result/weights-14.h5\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 392s 7ms/step - loss: 4.8379e-05 - capsnet_loss: 2.1375e-05 - decoder_loss: 0.0540 - capsnet_acc: 0.6587 - val_loss: 2.7288e-05 - val_capsnet_loss: 6.0204e-07 - val_decoder_loss: 0.0534 - val_capsnet_acc: 0.7411\n",
      "\n",
      "Epoch 00015: val_capsnet_acc did not improve from 0.76840\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 3.2597e-05 - capsnet_loss: 5.9244e-06 - decoder_loss: 0.0533 - capsnet_acc: 0.6911 - val_loss: 2.6747e-05 - val_capsnet_loss: 3.7709e-07 - val_decoder_loss: 0.0527 - val_capsnet_acc: 0.7892\n",
      "\n",
      "Epoch 00016: val_capsnet_acc improved from 0.76840 to 0.78920, saving model to ./result/weights-16.h5\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.8527e-05 - capsnet_loss: 2.0159e-06 - decoder_loss: 0.0530 - capsnet_acc: 0.7388 - val_loss: 2.6531e-05 - val_capsnet_loss: 2.5963e-07 - val_decoder_loss: 0.0525 - val_capsnet_acc: 0.8215\n",
      "\n",
      "Epoch 00017: val_capsnet_acc improved from 0.78920 to 0.82150, saving model to ./result/weights-17.h5\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.6710e-05 - capsnet_loss: 2.7695e-07 - decoder_loss: 0.0529 - capsnet_acc: 0.7864 - val_loss: 2.6429e-05 - val_capsnet_loss: 2.3184e-07 - val_decoder_loss: 0.0524 - val_capsnet_acc: 0.8132\n",
      "\n",
      "Epoch 00018: val_capsnet_acc did not improve from 0.82150\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 3.0869e-05 - capsnet_loss: 4.4656e-06 - decoder_loss: 0.0528 - capsnet_acc: 0.7336 - val_loss: 2.6418e-05 - val_capsnet_loss: 2.4276e-07 - val_decoder_loss: 0.0524 - val_capsnet_acc: 0.8486\n",
      "\n",
      "Epoch 00019: val_capsnet_acc improved from 0.82150 to 0.84860, saving model to ./result/weights-19.h5\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 3.2651e-05 - capsnet_loss: 6.2831e-06 - decoder_loss: 0.0527 - capsnet_acc: 0.7211 - val_loss: 2.6619e-05 - val_capsnet_loss: 5.4923e-07 - val_decoder_loss: 0.0521 - val_capsnet_acc: 0.8528\n",
      "\n",
      "Epoch 00020: val_capsnet_acc improved from 0.84860 to 0.85280, saving model to ./result/weights-20.h5\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 2.8361e-05 - capsnet_loss: 2.1154e-06 - decoder_loss: 0.0525 - capsnet_acc: 0.7914 - val_loss: 2.7013e-05 - val_capsnet_loss: 9.7558e-07 - val_decoder_loss: 0.0521 - val_capsnet_acc: 0.8129\n",
      "\n",
      "Epoch 00021: val_capsnet_acc did not improve from 0.85280\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 5.3504e-05 - capsnet_loss: 2.7243e-05 - decoder_loss: 0.0525 - capsnet_acc: 0.7407 - val_loss: 3.2510e-05 - val_capsnet_loss: 6.5616e-06 - val_decoder_loss: 0.0519 - val_capsnet_acc: 0.8053\n",
      "\n",
      "Epoch 00022: val_capsnet_acc did not improve from 0.85280\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.6986e-05 - capsnet_loss: 1.1071e-06 - decoder_loss: 0.0518 - capsnet_acc: 0.8611 - val_loss: 2.6246e-05 - val_capsnet_loss: 6.8080e-07 - val_decoder_loss: 0.0511 - val_capsnet_acc: 0.8479\n",
      "\n",
      "Epoch 00023: val_capsnet_acc did not improve from 0.85280\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.6189e-05 - capsnet_loss: 5.3529e-07 - decoder_loss: 0.0513 - capsnet_acc: 0.8591 - val_loss: 2.5927e-05 - val_capsnet_loss: 5.4742e-07 - val_decoder_loss: 0.0508 - val_capsnet_acc: 0.8628\n",
      "\n",
      "Epoch 00024: val_capsnet_acc improved from 0.85280 to 0.86280, saving model to ./result/weights-24.h5\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.6409e-05 - capsnet_loss: 9.9450e-07 - decoder_loss: 0.0508 - capsnet_acc: 0.8484 - val_loss: 2.5586e-05 - val_capsnet_loss: 5.0303e-07 - val_decoder_loss: 0.0502 - val_capsnet_acc: 0.8552\n",
      "\n",
      "Epoch 00025: val_capsnet_acc did not improve from 0.86280\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.7424e-05 - capsnet_loss: 2.2889e-06 - decoder_loss: 0.0503 - capsnet_acc: 0.8435 - val_loss: 2.5567e-05 - val_capsnet_loss: 8.1096e-07 - val_decoder_loss: 0.0495 - val_capsnet_acc: 0.8879\n",
      "\n",
      "Epoch 00026: val_capsnet_acc improved from 0.86280 to 0.88790, saving model to ./result/weights-26.h5\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.6214e-05 - capsnet_loss: 1.4180e-06 - decoder_loss: 0.0496 - capsnet_acc: 0.8643 - val_loss: 2.5507e-05 - val_capsnet_loss: 1.1114e-06 - val_decoder_loss: 0.0488 - val_capsnet_acc: 0.8657\n",
      "\n",
      "Epoch 00027: val_capsnet_acc did not improve from 0.88790\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.9503e-05 - capsnet_loss: 4.9908e-06 - decoder_loss: 0.0490 - capsnet_acc: 0.8477 - val_loss: 2.5123e-05 - val_capsnet_loss: 1.1031e-06 - val_decoder_loss: 0.0480 - val_capsnet_acc: 0.8834\n",
      "\n",
      "Epoch 00028: val_capsnet_acc did not improve from 0.88790\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.4828e-05 - capsnet_loss: 8.3067e-07 - decoder_loss: 0.0480 - capsnet_acc: 0.8916 - val_loss: 2.4109e-05 - val_capsnet_loss: 5.0291e-07 - val_decoder_loss: 0.0472 - val_capsnet_acc: 0.9199\n",
      "\n",
      "Epoch 00029: val_capsnet_acc improved from 0.88790 to 0.91990, saving model to ./result/weights-29.h5\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.4703e-05 - capsnet_loss: 1.1051e-06 - decoder_loss: 0.0472 - capsnet_acc: 0.8876 - val_loss: 2.4676e-05 - val_capsnet_loss: 1.4631e-06 - val_decoder_loss: 0.0464 - val_capsnet_acc: 0.8757\n",
      "\n",
      "Epoch 00030: val_capsnet_acc did not improve from 0.91990\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.5318e-05 - capsnet_loss: 2.0517e-06 - decoder_loss: 0.0465 - capsnet_acc: 0.8852 - val_loss: 2.3558e-05 - val_capsnet_loss: 7.3259e-07 - val_decoder_loss: 0.0457 - val_capsnet_acc: 0.9244\n",
      "\n",
      "Epoch 00031: val_capsnet_acc improved from 0.91990 to 0.92440, saving model to ./result/weights-31.h5\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.4369e-05 - capsnet_loss: 1.4877e-06 - decoder_loss: 0.0458 - capsnet_acc: 0.9050 - val_loss: 2.3348e-05 - val_capsnet_loss: 9.3361e-07 - val_decoder_loss: 0.0448 - val_capsnet_acc: 0.8935\n",
      "\n",
      "Epoch 00032: val_capsnet_acc did not improve from 0.92440\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.5930e-05 - capsnet_loss: 3.2506e-06 - decoder_loss: 0.0454 - capsnet_acc: 0.8929 - val_loss: 2.3302e-05 - val_capsnet_loss: 1.0447e-06 - val_decoder_loss: 0.0445 - val_capsnet_acc: 0.9076\n",
      "\n",
      "Epoch 00033: val_capsnet_acc did not improve from 0.92440\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.2951e-05 - capsnet_loss: 8.5973e-07 - decoder_loss: 0.0442 - capsnet_acc: 0.9195 - val_loss: 2.2401e-05 - val_capsnet_loss: 7.3234e-07 - val_decoder_loss: 0.0433 - val_capsnet_acc: 0.9363\n",
      "\n",
      "Epoch 00034: val_capsnet_acc improved from 0.92440 to 0.93630, saving model to ./result/weights-34.h5\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.2700e-05 - capsnet_loss: 1.0751e-06 - decoder_loss: 0.0433 - capsnet_acc: 0.9200 - val_loss: 2.1715e-05 - val_capsnet_loss: 5.9269e-07 - val_decoder_loss: 0.0422 - val_capsnet_acc: 0.9381\n",
      "\n",
      "Epoch 00035: val_capsnet_acc improved from 0.93630 to 0.93810, saving model to ./result/weights-35.h5\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.3820e-05 - capsnet_loss: 2.5497e-06 - decoder_loss: 0.0425 - capsnet_acc: 0.9142 - val_loss: 2.1875e-05 - val_capsnet_loss: 1.0570e-06 - val_decoder_loss: 0.0416 - val_capsnet_acc: 0.9522\n",
      "\n",
      "Epoch 00036: val_capsnet_acc improved from 0.93810 to 0.95220, saving model to ./result/weights-36.h5\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.1840e-05 - capsnet_loss: 1.1301e-06 - decoder_loss: 0.0414 - capsnet_acc: 0.9316 - val_loss: 2.1600e-05 - val_capsnet_loss: 1.3540e-06 - val_decoder_loss: 0.0405 - val_capsnet_acc: 0.9101\n",
      "\n",
      "Epoch 00037: val_capsnet_acc did not improve from 0.95220\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.1619e-05 - capsnet_loss: 1.3645e-06 - decoder_loss: 0.0405 - capsnet_acc: 0.9291 - val_loss: 2.1043e-05 - val_capsnet_loss: 1.2332e-06 - val_decoder_loss: 0.0396 - val_capsnet_acc: 0.9257\n",
      "\n",
      "Epoch 00038: val_capsnet_acc did not improve from 0.95220\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.0629e-05 - capsnet_loss: 8.6520e-07 - decoder_loss: 0.0395 - capsnet_acc: 0.9415 - val_loss: 2.0607e-05 - val_capsnet_loss: 1.2333e-06 - val_decoder_loss: 0.0387 - val_capsnet_acc: 0.9341\n",
      "\n",
      "Epoch 00039: val_capsnet_acc did not improve from 0.95220\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.1218e-05 - capsnet_loss: 1.7028e-06 - decoder_loss: 0.0390 - capsnet_acc: 0.9326 - val_loss: 2.0332e-05 - val_capsnet_loss: 1.1462e-06 - val_decoder_loss: 0.0384 - val_capsnet_acc: 0.9507\n",
      "\n",
      "Epoch 00040: val_capsnet_acc did not improve from 0.95220\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.0378e-05 - capsnet_loss: 1.3523e-06 - decoder_loss: 0.0381 - capsnet_acc: 0.9405 - val_loss: 2.0283e-05 - val_capsnet_loss: 1.5981e-06 - val_decoder_loss: 0.0374 - val_capsnet_acc: 0.9252\n",
      "\n",
      "Epoch 00041: val_capsnet_acc did not improve from 0.95220\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.9923e-05 - capsnet_loss: 1.2756e-06 - decoder_loss: 0.0373 - capsnet_acc: 0.9414 - val_loss: 1.9575e-05 - val_capsnet_loss: 1.2417e-06 - val_decoder_loss: 0.0367 - val_capsnet_acc: 0.9542\n",
      "\n",
      "Epoch 00042: val_capsnet_acc improved from 0.95220 to 0.95420, saving model to ./result/weights-42.h5\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.9138e-05 - capsnet_loss: 9.2476e-07 - decoder_loss: 0.0364 - capsnet_acc: 0.9494 - val_loss: 1.8813e-05 - val_capsnet_loss: 9.3712e-07 - val_decoder_loss: 0.0358 - val_capsnet_acc: 0.9494\n",
      "\n",
      "Epoch 00043: val_capsnet_acc did not improve from 0.95420\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.8731e-05 - capsnet_loss: 8.8209e-07 - decoder_loss: 0.0357 - capsnet_acc: 0.9508 - val_loss: 1.8322e-05 - val_capsnet_loss: 8.9641e-07 - val_decoder_loss: 0.0349 - val_capsnet_acc: 0.9550\n",
      "\n",
      "Epoch 00044: val_capsnet_acc improved from 0.95420 to 0.95500, saving model to ./result/weights-44.h5\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.9694e-05 - capsnet_loss: 2.0486e-06 - decoder_loss: 0.0353 - capsnet_acc: 0.9411 - val_loss: 1.8846e-05 - val_capsnet_loss: 1.6208e-06 - val_decoder_loss: 0.0345 - val_capsnet_acc: 0.9523\n",
      "\n",
      "Epoch 00045: val_capsnet_acc did not improve from 0.95500\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.8376e-05 - capsnet_loss: 1.1514e-06 - decoder_loss: 0.0345 - capsnet_acc: 0.9537 - val_loss: 1.7778e-05 - val_capsnet_loss: 8.8761e-07 - val_decoder_loss: 0.0338 - val_capsnet_acc: 0.9681\n",
      "\n",
      "Epoch 00046: val_capsnet_acc improved from 0.95500 to 0.96810, saving model to ./result/weights-46.h5\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.8193e-05 - capsnet_loss: 1.2477e-06 - decoder_loss: 0.0339 - capsnet_acc: 0.9511 - val_loss: 1.7806e-05 - val_capsnet_loss: 1.1919e-06 - val_decoder_loss: 0.0332 - val_capsnet_acc: 0.9536\n",
      "\n",
      "Epoch 00047: val_capsnet_acc did not improve from 0.96810\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 1.7880e-05 - capsnet_loss: 1.2418e-06 - decoder_loss: 0.0333 - capsnet_acc: 0.9547 - val_loss: 1.7833e-05 - val_capsnet_loss: 1.5410e-06 - val_decoder_loss: 0.0326 - val_capsnet_acc: 0.9582\n",
      "\n",
      "Epoch 00048: val_capsnet_acc did not improve from 0.96810\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.8203e-05 - capsnet_loss: 1.7113e-06 - decoder_loss: 0.0330 - capsnet_acc: 0.9547 - val_loss: 1.7048e-05 - val_capsnet_loss: 1.1085e-06 - val_decoder_loss: 0.0319 - val_capsnet_acc: 0.9643\n",
      "\n",
      "Epoch 00049: val_capsnet_acc did not improve from 0.96810\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.7324e-05 - capsnet_loss: 1.2584e-06 - decoder_loss: 0.0321 - capsnet_acc: 0.9580 - val_loss: 1.7677e-05 - val_capsnet_loss: 1.8785e-06 - val_decoder_loss: 0.0316 - val_capsnet_acc: 0.9431\n",
      "\n",
      "Epoch 00050: val_capsnet_acc did not improve from 0.96810\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 391s 7ms/step - loss: 1.6782e-05 - capsnet_loss: 1.0622e-06 - decoder_loss: 0.0314 - capsnet_acc: 0.9622 - val_loss: 1.7204e-05 - val_capsnet_loss: 1.5930e-06 - val_decoder_loss: 0.0312 - val_capsnet_acc: 0.9591\n",
      "\n",
      "Epoch 00001: val_capsnet_acc did not improve from 0.96810\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 2.1386e-05 - capsnet_loss: 5.0439e-06 - decoder_loss: 0.0327 - capsnet_acc: 0.9481 - val_loss: 1.6601e-05 - val_capsnet_loss: 1.2712e-06 - val_decoder_loss: 0.0307 - val_capsnet_acc: 0.9644\n",
      "\n",
      "Epoch 00002: val_capsnet_acc did not improve from 0.96810\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.6215e-05 - capsnet_loss: 8.8331e-07 - decoder_loss: 0.0307 - capsnet_acc: 0.9675 - val_loss: 1.7881e-05 - val_capsnet_loss: 2.9299e-06 - val_decoder_loss: 0.0299 - val_capsnet_acc: 0.9450\n",
      "\n",
      "Epoch 00003: val_capsnet_acc did not improve from 0.96810\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.6001e-05 - capsnet_loss: 9.9019e-07 - decoder_loss: 0.0300 - capsnet_acc: 0.9673 - val_loss: 1.6010e-05 - val_capsnet_loss: 1.3819e-06 - val_decoder_loss: 0.0293 - val_capsnet_acc: 0.9664\n",
      "\n",
      "Epoch 00004: val_capsnet_acc did not improve from 0.96810\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.5678e-05 - capsnet_loss: 9.2479e-07 - decoder_loss: 0.0295 - capsnet_acc: 0.9684 - val_loss: 1.5719e-05 - val_capsnet_loss: 1.3351e-06 - val_decoder_loss: 0.0288 - val_capsnet_acc: 0.9689\n",
      "\n",
      "Epoch 00005: val_capsnet_acc improved from 0.96810 to 0.96890, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.5788e-05 - capsnet_loss: 1.2102e-06 - decoder_loss: 0.0292 - capsnet_acc: 0.9670 - val_loss: 1.5161e-05 - val_capsnet_loss: 1.0154e-06 - val_decoder_loss: 0.0283 - val_capsnet_acc: 0.9716\n",
      "\n",
      "Epoch 00006: val_capsnet_acc improved from 0.96890 to 0.97160, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.5003e-05 - capsnet_loss: 7.5393e-07 - decoder_loss: 0.0285 - capsnet_acc: 0.9720 - val_loss: 1.4966e-05 - val_capsnet_loss: 1.0310e-06 - val_decoder_loss: 0.0279 - val_capsnet_acc: 0.9690\n",
      "\n",
      "Epoch 00007: val_capsnet_acc did not improve from 0.97160\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.5236e-05 - capsnet_loss: 1.1205e-06 - decoder_loss: 0.0282 - capsnet_acc: 0.9698 - val_loss: 1.4818e-05 - val_capsnet_loss: 1.1302e-06 - val_decoder_loss: 0.0274 - val_capsnet_acc: 0.9694\n",
      "\n",
      "Epoch 00008: val_capsnet_acc did not improve from 0.97160\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.5158e-05 - capsnet_loss: 1.1781e-06 - decoder_loss: 0.0280 - capsnet_acc: 0.9696 - val_loss: 1.6360e-05 - val_capsnet_loss: 2.4138e-06 - val_decoder_loss: 0.0279 - val_capsnet_acc: 0.9721\n",
      "\n",
      "Epoch 00009: val_capsnet_acc improved from 0.97160 to 0.97210, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.5001e-05 - capsnet_loss: 1.2324e-06 - decoder_loss: 0.0275 - capsnet_acc: 0.9693 - val_loss: 1.5325e-05 - val_capsnet_loss: 1.9577e-06 - val_decoder_loss: 0.0267 - val_capsnet_acc: 0.9668\n",
      "\n",
      "Epoch 00010: val_capsnet_acc did not improve from 0.97210\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.4689e-05 - capsnet_loss: 1.1166e-06 - decoder_loss: 0.0271 - capsnet_acc: 0.9723 - val_loss: 1.5784e-05 - val_capsnet_loss: 2.3457e-06 - val_decoder_loss: 0.0269 - val_capsnet_acc: 0.9655\n",
      "\n",
      "Epoch 00011: val_capsnet_acc did not improve from 0.97210\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.5574e-05 - capsnet_loss: 1.9904e-06 - decoder_loss: 0.0272 - capsnet_acc: 0.9692 - val_loss: 1.5597e-05 - val_capsnet_loss: 2.2693e-06 - val_decoder_loss: 0.0267 - val_capsnet_acc: 0.9640\n",
      "\n",
      "Epoch 00012: val_capsnet_acc did not improve from 0.97210\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.4183e-05 - capsnet_loss: 9.7994e-07 - decoder_loss: 0.0264 - capsnet_acc: 0.9766 - val_loss: 2.1912e-05 - val_capsnet_loss: 8.7963e-06 - val_decoder_loss: 0.0262 - val_capsnet_acc: 0.9257\n",
      "\n",
      "Epoch 00013: val_capsnet_acc did not improve from 0.97210\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.4024e-05 - capsnet_loss: 9.8963e-07 - decoder_loss: 0.0261 - capsnet_acc: 0.9767 - val_loss: 1.5342e-05 - val_capsnet_loss: 2.2790e-06 - val_decoder_loss: 0.0261 - val_capsnet_acc: 0.9713\n",
      "\n",
      "Epoch 00014: val_capsnet_acc did not improve from 0.97210\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.4746e-05 - capsnet_loss: 1.7089e-06 - decoder_loss: 0.0261 - capsnet_acc: 0.9744 - val_loss: 1.3900e-05 - val_capsnet_loss: 1.3439e-06 - val_decoder_loss: 0.0251 - val_capsnet_acc: 0.9789\n",
      "\n",
      "Epoch 00015: val_capsnet_acc improved from 0.97210 to 0.97890, saving model to ./result/weights-15.h5\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.4258e-05 - capsnet_loss: 1.3849e-06 - decoder_loss: 0.0257 - capsnet_acc: 0.9774 - val_loss: 1.5366e-05 - val_capsnet_loss: 2.6576e-06 - val_decoder_loss: 0.0254 - val_capsnet_acc: 0.9624\n",
      "\n",
      "Epoch 00016: val_capsnet_acc did not improve from 0.97890\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.3371e-05 - capsnet_loss: 8.4137e-07 - decoder_loss: 0.0251 - capsnet_acc: 0.9798 - val_loss: 1.4023e-05 - val_capsnet_loss: 1.7693e-06 - val_decoder_loss: 0.0245 - val_capsnet_acc: 0.9744\n",
      "\n",
      "Epoch 00017: val_capsnet_acc did not improve from 0.97890\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.3144e-05 - capsnet_loss: 7.9865e-07 - decoder_loss: 0.0247 - capsnet_acc: 0.9813 - val_loss: 1.3721e-05 - val_capsnet_loss: 1.4811e-06 - val_decoder_loss: 0.0245 - val_capsnet_acc: 0.9784\n",
      "\n",
      "Epoch 00018: val_capsnet_acc did not improve from 0.97890\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.3102e-05 - capsnet_loss: 8.5065e-07 - decoder_loss: 0.0245 - capsnet_acc: 0.9810 - val_loss: 1.3205e-05 - val_capsnet_loss: 1.3718e-06 - val_decoder_loss: 0.0237 - val_capsnet_acc: 0.9793\n",
      "\n",
      "Epoch 00019: val_capsnet_acc improved from 0.97890 to 0.97930, saving model to ./result/weights-19.h5\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.3555e-05 - capsnet_loss: 1.3466e-06 - decoder_loss: 0.0244 - capsnet_acc: 0.9775 - val_loss: 1.3484e-05 - val_capsnet_loss: 1.7511e-06 - val_decoder_loss: 0.0235 - val_capsnet_acc: 0.9787\n",
      "\n",
      "Epoch 00020: val_capsnet_acc did not improve from 0.97930\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 1.3785e-05 - capsnet_loss: 1.5785e-06 - decoder_loss: 0.0244 - capsnet_acc: 0.9774 - val_loss: 1.4551e-05 - val_capsnet_loss: 2.5292e-06 - val_decoder_loss: 0.0240 - val_capsnet_acc: 0.9668\n",
      "\n",
      "Epoch 00021: val_capsnet_acc did not improve from 0.97930\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 1.6455e-05 - capsnet_loss: 3.8234e-06 - decoder_loss: 0.0253 - capsnet_acc: 0.9726 - val_loss: 1.9653e-05 - val_capsnet_loss: 6.6890e-06 - val_decoder_loss: 0.0259 - val_capsnet_acc: 0.9663\n",
      "\n",
      "Epoch 00022: val_capsnet_acc did not improve from 0.97930\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.6034e-05 - capsnet_loss: 3.4251e-06 - decoder_loss: 0.0252 - capsnet_acc: 0.9760 - val_loss: 1.4547e-05 - val_capsnet_loss: 2.7773e-06 - val_decoder_loss: 0.0235 - val_capsnet_acc: 0.9776\n",
      "\n",
      "Epoch 00023: val_capsnet_acc did not improve from 0.97930\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 395s 7ms/step - loss: 1.2665e-05 - capsnet_loss: 9.7633e-07 - decoder_loss: 0.0234 - capsnet_acc: 0.9838 - val_loss: 1.3373e-05 - val_capsnet_loss: 2.0823e-06 - val_decoder_loss: 0.0226 - val_capsnet_acc: 0.9838\n",
      "\n",
      "Epoch 00024: val_capsnet_acc improved from 0.97930 to 0.98380, saving model to ./result/weights-24.h5\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 393s 7ms/step - loss: 1.2669e-05 - capsnet_loss: 1.1226e-06 - decoder_loss: 0.0231 - capsnet_acc: 0.9838 - val_loss: 1.3374e-05 - val_capsnet_loss: 2.0149e-06 - val_decoder_loss: 0.0227 - val_capsnet_acc: 0.9777\n",
      "\n",
      "Epoch 00025: val_capsnet_acc did not improve from 0.98380\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 399s 7ms/step - loss: 1.2597e-05 - capsnet_loss: 1.2102e-06 - decoder_loss: 0.0228 - capsnet_acc: 0.9849 - val_loss: 1.2722e-05 - val_capsnet_loss: 1.7219e-06 - val_decoder_loss: 0.0220 - val_capsnet_acc: 0.9785\n",
      "\n",
      "Epoch 00026: val_capsnet_acc did not improve from 0.98380\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 381s 6ms/step - loss: 1.1991e-05 - capsnet_loss: 7.4635e-07 - decoder_loss: 0.0225 - capsnet_acc: 0.9861 - val_loss: 1.2475e-05 - val_capsnet_loss: 1.6362e-06 - val_decoder_loss: 0.0217 - val_capsnet_acc: 0.9854\n",
      "\n",
      "Epoch 00027: val_capsnet_acc improved from 0.98380 to 0.98540, saving model to ./result/weights-27.h5\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 387s 6ms/step - loss: 1.1990e-05 - capsnet_loss: 8.5532e-07 - decoder_loss: 0.0223 - capsnet_acc: 0.9863 - val_loss: 1.2167e-05 - val_capsnet_loss: 1.4737e-06 - val_decoder_loss: 0.0214 - val_capsnet_acc: 0.9844\n",
      "\n",
      "Epoch 00028: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 402s 7ms/step - loss: 1.1687e-05 - capsnet_loss: 6.9629e-07 - decoder_loss: 0.0220 - capsnet_acc: 0.9867 - val_loss: 1.3093e-05 - val_capsnet_loss: 2.3675e-06 - val_decoder_loss: 0.0215 - val_capsnet_acc: 0.9828\n",
      "\n",
      "Epoch 00029: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 408s 7ms/step - loss: 1.1583e-05 - capsnet_loss: 7.0519e-07 - decoder_loss: 0.0218 - capsnet_acc: 0.9867 - val_loss: 1.2764e-05 - val_capsnet_loss: 2.2865e-06 - val_decoder_loss: 0.0210 - val_capsnet_acc: 0.9837\n",
      "\n",
      "Epoch 00030: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 402s 7ms/step - loss: 1.2392e-05 - capsnet_loss: 1.3788e-06 - decoder_loss: 0.0220 - capsnet_acc: 0.9845 - val_loss: 1.3187e-05 - val_capsnet_loss: 2.4813e-06 - val_decoder_loss: 0.0214 - val_capsnet_acc: 0.9822\n",
      "\n",
      "Epoch 00031: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 399s 7ms/step - loss: 1.3541e-05 - capsnet_loss: 2.2849e-06 - decoder_loss: 0.0225 - capsnet_acc: 0.9819 - val_loss: 1.4018e-05 - val_capsnet_loss: 3.3476e-06 - val_decoder_loss: 0.0213 - val_capsnet_acc: 0.9820\n",
      "\n",
      "Epoch 00032: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 397s 7ms/step - loss: 1.1746e-05 - capsnet_loss: 9.8004e-07 - decoder_loss: 0.0215 - capsnet_acc: 0.9871 - val_loss: 1.4937e-05 - val_capsnet_loss: 4.4063e-06 - val_decoder_loss: 0.0211 - val_capsnet_acc: 0.9821\n",
      "\n",
      "Epoch 00033: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 394s 7ms/step - loss: 1.2101e-05 - capsnet_loss: 1.2652e-06 - decoder_loss: 0.0217 - capsnet_acc: 0.9876 - val_loss: 1.3101e-05 - val_capsnet_loss: 2.6492e-06 - val_decoder_loss: 0.0209 - val_capsnet_acc: 0.9811\n",
      "\n",
      "Epoch 00034: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 377s 6ms/step - loss: 1.1301e-05 - capsnet_loss: 8.3379e-07 - decoder_loss: 0.0209 - capsnet_acc: 0.9884 - val_loss: 1.3361e-05 - val_capsnet_loss: 2.9968e-06 - val_decoder_loss: 0.0207 - val_capsnet_acc: 0.9821\n",
      "\n",
      "Epoch 00035: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 400s 7ms/step - loss: 1.1558e-05 - capsnet_loss: 1.1455e-06 - decoder_loss: 0.0208 - capsnet_acc: 0.9883 - val_loss: 1.2908e-05 - val_capsnet_loss: 2.6029e-06 - val_decoder_loss: 0.0206 - val_capsnet_acc: 0.9825\n",
      "\n",
      "Epoch 00036: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 370s 6ms/step - loss: 1.5653e-05 - capsnet_loss: 4.7140e-06 - decoder_loss: 0.0219 - capsnet_acc: 0.9802 - val_loss: 1.4044e-05 - val_capsnet_loss: 3.5006e-06 - val_decoder_loss: 0.0211 - val_capsnet_acc: 0.9811\n",
      "\n",
      "Epoch 00037: val_capsnet_acc did not improve from 0.98540\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 357s 6ms/step - loss: 1.1066e-05 - capsnet_loss: 7.8881e-07 - decoder_loss: 0.0206 - capsnet_acc: 0.9911 - val_loss: 1.1972e-05 - val_capsnet_loss: 2.1231e-06 - val_decoder_loss: 0.0197 - val_capsnet_acc: 0.9864\n",
      "\n",
      "Epoch 00038: val_capsnet_acc improved from 0.98540 to 0.98640, saving model to ./result/weights-38.h5\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 388s 6ms/step - loss: 1.0752e-05 - capsnet_loss: 6.0899e-07 - decoder_loss: 0.0203 - capsnet_acc: 0.9924 - val_loss: 1.3548e-05 - val_capsnet_loss: 3.5468e-06 - val_decoder_loss: 0.0200 - val_capsnet_acc: 0.9834\n",
      "\n",
      "Epoch 00039: val_capsnet_acc did not improve from 0.98640\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 392s 7ms/step - loss: 1.1102e-05 - capsnet_loss: 9.5546e-07 - decoder_loss: 0.0203 - capsnet_acc: 0.9899 - val_loss: 1.2817e-05 - val_capsnet_loss: 2.7572e-06 - val_decoder_loss: 0.0201 - val_capsnet_acc: 0.9865\n",
      "\n",
      "Epoch 00040: val_capsnet_acc improved from 0.98640 to 0.98650, saving model to ./result/weights-40.h5\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 358s 6ms/step - loss: 1.0603e-05 - capsnet_loss: 6.3892e-07 - decoder_loss: 0.0199 - capsnet_acc: 0.9916 - val_loss: 1.1505e-05 - val_capsnet_loss: 1.9744e-06 - val_decoder_loss: 0.0191 - val_capsnet_acc: 0.9873\n",
      "\n",
      "Epoch 00041: val_capsnet_acc improved from 0.98650 to 0.98730, saving model to ./result/weights-41.h5\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 357s 6ms/step - loss: 1.0002e-05 - capsnet_loss: 2.7584e-07 - decoder_loss: 0.0195 - capsnet_acc: 0.9943 - val_loss: 1.1814e-05 - val_capsnet_loss: 2.3967e-06 - val_decoder_loss: 0.0188 - val_capsnet_acc: 0.9883\n",
      "\n",
      "Epoch 00042: val_capsnet_acc improved from 0.98730 to 0.98830, saving model to ./result/weights-42.h5\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 363s 6ms/step - loss: 1.0216e-05 - capsnet_loss: 4.9765e-07 - decoder_loss: 0.0194 - capsnet_acc: 0.9930 - val_loss: 1.2603e-05 - val_capsnet_loss: 3.0205e-06 - val_decoder_loss: 0.0192 - val_capsnet_acc: 0.9857\n",
      "\n",
      "Epoch 00043: val_capsnet_acc did not improve from 0.98830\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 355s 6ms/step - loss: 1.1105e-05 - capsnet_loss: 1.2379e-06 - decoder_loss: 0.0197 - capsnet_acc: 0.9898 - val_loss: 1.9390e-05 - val_capsnet_loss: 9.2018e-06 - val_decoder_loss: 0.0204 - val_capsnet_acc: 0.9756\n",
      "\n",
      "Epoch 00044: val_capsnet_acc did not improve from 0.98830\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 357s 6ms/step - loss: 1.0603e-05 - capsnet_loss: 8.8235e-07 - decoder_loss: 0.0194 - capsnet_acc: 0.9920 - val_loss: 1.1497e-05 - val_capsnet_loss: 2.1612e-06 - val_decoder_loss: 0.0187 - val_capsnet_acc: 0.9861\n",
      "\n",
      "Epoch 00045: val_capsnet_acc did not improve from 0.98830\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 357s 6ms/step - loss: 1.0600e-05 - capsnet_loss: 9.4995e-07 - decoder_loss: 0.0193 - capsnet_acc: 0.9923 - val_loss: 1.2425e-05 - val_capsnet_loss: 2.7595e-06 - val_decoder_loss: 0.0193 - val_capsnet_acc: 0.9839\n",
      "\n",
      "Epoch 00046: val_capsnet_acc did not improve from 0.98830\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 370s 6ms/step - loss: 1.0525e-05 - capsnet_loss: 9.3018e-07 - decoder_loss: 0.0192 - capsnet_acc: 0.9915 - val_loss: 1.2351e-05 - val_capsnet_loss: 2.9370e-06 - val_decoder_loss: 0.0188 - val_capsnet_acc: 0.9867\n",
      "\n",
      "Epoch 00047: val_capsnet_acc did not improve from 0.98830\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 413s 7ms/step - loss: 1.0059e-05 - capsnet_loss: 6.1842e-07 - decoder_loss: 0.0189 - capsnet_acc: 0.9930 - val_loss: 1.1931e-05 - val_capsnet_loss: 2.6520e-06 - val_decoder_loss: 0.0186 - val_capsnet_acc: 0.9856\n",
      "\n",
      "Epoch 00048: val_capsnet_acc did not improve from 0.98830\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 410s 7ms/step - loss: 9.9023e-06 - capsnet_loss: 5.4598e-07 - decoder_loss: 0.0187 - capsnet_acc: 0.9939 - val_loss: 1.1343e-05 - val_capsnet_loss: 2.4180e-06 - val_decoder_loss: 0.0178 - val_capsnet_acc: 0.9897\n",
      "\n",
      "Epoch 00049: val_capsnet_acc improved from 0.98830 to 0.98970, saving model to ./result/weights-49.h5\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 405s 7ms/step - loss: 9.6857e-06 - capsnet_loss: 4.5233e-07 - decoder_loss: 0.0185 - capsnet_acc: 0.9942 - val_loss: 1.1123e-05 - val_capsnet_loss: 2.2233e-06 - val_decoder_loss: 0.0178 - val_capsnet_acc: 0.9895\n",
      "\n",
      "Epoch 00050: val_capsnet_acc did not improve from 0.98970\n",
      "Trained model saved to \\ ./result/trained_model.h5'\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 100\n",
    "routings = 3\n",
    "shift_fraction = 0.1\n",
    "testing = False\n",
    "debug = True\n",
    "save_dir = './result'\n",
    "weights = None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import argparse\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    from keras import callbacks\n",
    "\n",
    "    # setting the hyper parameters\n",
    "    #parser = argparse.ArgumentParser(description=\"Capsule Network on MNIST.\")\n",
    "    #parser.add_argument('--digit', default=5, type=int,\n",
    "    #                    help=\"Digit to manipulate\")\n",
    "\n",
    "    #args = parser.parse_args()\n",
    "    #print(args)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # load data\n",
    "    (x_train, y_train), (x_test, y_test) = load_mnist()\n",
    "\n",
    "    # define model\n",
    "    model, eval_model = CapsNet(input_shape=x_train.shape[1:],\n",
    "                                                  n_class=len(np.unique(np.argmax(y_train, 1))),\n",
    "                                                  routings=routings)\n",
    "    model.summary()\n",
    "\n",
    "    # train or test\n",
    "    if weights is not None:  # init the model weights with provided one\n",
    "        model.load_weights(weights)\n",
    "    if not testing:\n",
    "        train(model=model, data=((x_train, y_train), (x_test, y_test)), save_dir=save_dir)\n",
    "    else:  # as long as weights are given, will run testing\n",
    "        if weights is None:\n",
    "            print('No weights are provided. Will test using random initialized weights.')\n",
    "        #manipulate_latent(manipulate_model, (x_test, y_test), args)\n",
    "        test(model=eval_model, data=(x_test, y_test), save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CapsNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
