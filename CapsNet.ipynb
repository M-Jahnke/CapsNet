{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4X4-oRRf_Xd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZRyA-qEYWB2",
        "colab_type": "code",
        "outputId": "30141124-1b3c-4668-a902-eb813d9b28ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import layers, models, optimizers, initializers\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "print(\"backend:\", K.backend())\n",
        "tf_session = K.get_session()\n",
        "#from utils import plot_log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "backend: tensorflow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPI2Sh3oj9O2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squash(vector, axis=-1):\n",
        "  squared_norm = K.sum(K.square(vector), axis, keepdims=True)\n",
        "  scale = squared_norm / (1 + squared_norm) / K.sqrt(squared_norm + K.epsilon())\n",
        "  return vector * scale\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9L77mKQgQEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PrimaryCapsules(inputs, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid'):\n",
        "  output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, \n",
        "                       padding=padding, name='primarycap_conv2d')(inputs)\n",
        "  outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
        "  return layers.Lambda(squash, name='primarycap_squash')(outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOTjwhy05IDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsuleLayer(layers.Layer):\n",
        "  def __init__(self, num_capsule, dim_capsule, routings=3, kernel_initializer='glorot_uniform', **kwargs):\n",
        "    super(CapsuleLayer, self).__init__(**kwargs)\n",
        "    self.num_capsule = num_capsule\n",
        "    self.dim_capsule = dim_capsule\n",
        "    self.routings = routings\n",
        "    self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "    self.input_num_capsule = input_shape[1]\n",
        "    self.input_dim_capsule = input_shape[2]\n",
        "    \n",
        "    self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule, self.dim_capsule, self.input_dim_capsule], \n",
        "                            initializer=self.kernel_initializer, name='W')\n",
        "    \n",
        "    print(\"w shape: \", K.int_shape(self.W)) # (10, 1152, 16, 8)\n",
        "    \n",
        "    #self.W_init = tf.random_normal(shape=(1, 1152, 10, 16, 8),\n",
        "    #                         stddev=0.01, dtype=tf.float32, name=\"W_init\")\n",
        "    #self.W = tf.Variable(self.W_init, name=\"W\")\n",
        "    #self.W_tiled = tf.tile(self.W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
        "    #print(\"W_tiled shape: \", self.W_tiled.shape)\n",
        "    \n",
        "    self.built = True\n",
        "    \n",
        "  def call(self, inputs, training=None):\n",
        "    \n",
        "    print(\"inputs shape: \", K.int_shape(inputs)) # (None, 1152, 8)\n",
        "    \n",
        "    inputs_expand = K.expand_dims(inputs, 1)\n",
        "    #inputs_expand = layers.Lambda(lambda x: K.expand_dims(x, 1))(inputs)\n",
        "    print(\"inputs_expand shape: \", K.int_shape(inputs_expand)) # (None, 1, 1152, 8)\n",
        "    \n",
        "    inputs_tiled =  K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
        "    #inputs_tiled = layers.Lambda(lambda x: K.tile(x, [1, self.num_capsule, 1, 1]))(inputs_expand)\n",
        "    print(\"inputs_tiled shape: \", K.int_shape(inputs_tiled)) # (None, 10, 1152, 8)\n",
        "    \n",
        "    inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
        "    print(\"inputs_hat shape: \", K.int_shape(inputs_hat)) # (None, 10, None, 1152, 16)\n",
        "    \n",
        "    #inputs_expand = tf.expand_dims(inputs, -1, name=\"inputs_expand\")\n",
        "    #inputs_tile = tf.expand_dims(inputs_expand, 2, name=\"inputs_tile\")\n",
        "    #inputs_tiled = tf.tile(inputs_tile, [1, 1, 10, 1, 1], name=\"inputs_tiled\")\n",
        "    #print(\"inputs_tiled shape: \", inputs_tiled.shape)\n",
        "    #inputs_hat = tf.matmul(self.W_tiled, inputs_tiled, name=\"inputs_hat\") #prediction\n",
        "    #print(\"inputs_hat shape: \", inputs_hat.shape)\n",
        "    \n",
        "    \n",
        "    # Start: Routing algorithm\n",
        "    b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
        "    print(\"b (weights) shape: \", K.int_shape(b)) # (None, 10, 1152)\n",
        "    #raw_weights = tf.zeros([batch_size, 1152, 10, 1, 1], dtype=np.float32, name=\"raw_weights\")\n",
        "    \n",
        "    assert self.routings > 0, 'Routings should be > 0'\n",
        "    for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, dim=1)\n",
        "            #c = tf.nn.softmax(raw_weights, dim=2, name=\"c\")\n",
        "\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule] <-- nicht erfÃ¼llt!\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
        "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
        "            \n",
        "            #weighted_predictions = tf.multiply(c, inputs_hat, name=\"weighted_predictions\")\n",
        "            #weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True, name=\"weighted_sum\")\n",
        "            #outputs = squash(weighted_sum)\n",
        "            #print(\"outputs shape: \", outputs.shape)\n",
        "            #outputs_tiled = tf.tile(outputs, [1, 1152, 1, 1, 1], name=\"outputs_tiled\")\n",
        "            #print(\"outputs_tiled shape: \", outputs_tiled)\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # The first two dimensions as `batch` dimension,\n",
        "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
        "                \n",
        "                print(\"outputs shape: \", K.int_shape(outputs)) # (None, 10, 10, 1152, 16)\n",
        "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
        "                #raw_weights += tf.matmul(inputs_hat, outputs_tiled, transpose_a=True)\n",
        "                \n",
        "      # End: Routing algorithm\n",
        "\n",
        "    return outputs\n",
        "  \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "  \n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        'num_capsule': self.num_capsule,\n",
        "        'dim_capsule': self.dim_capsule,\n",
        "        'routings': self.routings\n",
        "    }\n",
        "    base_config = super(CapsuleLayer, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "  \n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UheC3AQh4sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Length(layers.Layer):\n",
        "  def call(self, inputs, **kwargs):\n",
        "      return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "      return input_shape[:-1]\n",
        "\n",
        "  def get_config(self):\n",
        "      config = super(Length, self).get_config()\n",
        "      return config  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOAd0CwriIpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mask(layers.Layer):\n",
        "  def call(self, inputs, **kwargs):\n",
        "    if type(inputs) is list: # label is provided with shape\n",
        "      assert len(inputs) == 2\n",
        "      inputs, mask = inputs\n",
        "    else:\n",
        "      x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "      mask = K.one_hot(indices=K.argmax(x, 1), num_classes = x.get_shape().as_list()[1])\n",
        "      \n",
        "    masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
        "    return masked\n",
        "  \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    if type(input_shape[0]) is tuple: #with class label\n",
        "      return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "    else:\n",
        "      return tuple([None, input_shape[1] * input_shape[2]])\n",
        "    \n",
        "  def get_config(self):\n",
        "    config = super(Mask, self).get_config()\n",
        "    return config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxpVlxtrk2gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CapsNet(input_shape, n_class, routings):\n",
        "  x = layers.Input(shape=input_shape)\n",
        "  \n",
        "  \n",
        "  # Start: Encoder ---------------------------\n",
        "  #Layer 1: Conv2D\n",
        "  conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "  \n",
        "  #Layer 2: Conv2D with squash activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "  primary_caps = PrimaryCapsules(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
        "  \n",
        "  #Layer 3: Capsule Layer, Routing here\n",
        "  digit_caps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings, name='digitcaps')(primary_caps)\n",
        "  \n",
        "  # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "  # If using tensorflow, this will not be necessary. \n",
        "  out_caps = Length(name='capsnet')(digit_caps)\n",
        "  # End: Encoder ---------------------------\n",
        "  \n",
        "  \n",
        "  #Start: Decoder --------------------------\n",
        "  y = layers.Input(shape=(n_class,))\n",
        "  masked_by_y = Mask()([digit_caps, y]) # The true label is used to mask the output of capsule layer. For training\n",
        "  masked = Mask()(digit_caps) # Mask using the capsule with maximal length. For prediction\n",
        "  \n",
        "  decoder = models.Sequential(name='decoder')\n",
        "  decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
        "  decoder.add(layers.Dense(1024, activation='relu'))\n",
        "  decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "  decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "  \n",
        "  #End: Decoder ----------------------------\n",
        "  \n",
        "  train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "  eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "  \n",
        "  #manipulate model\n",
        "  \n",
        "  #noise = layers.Input(shape=(n_class, 16))\n",
        "  #noised_digitcaps = layers.Add()([digit_caps, noise])\n",
        "  #masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "  #manipulate_model = models.Model([x,y,noise], decoder(masked_noised_y))\n",
        "  return train_model, eval_model#, manipulate_model\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAQncp2Hq3JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def margin_loss(y_true, y_pred):\n",
        "  L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + 0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.9))\n",
        "  return K.mean(K.sum(L, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rYLsH5ry0i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sum_squared_error(y_true, y_pred):\n",
        "    if not K.is_tensor(y_pred):\n",
        "        y_pred = K.constant(y_pred)\n",
        "    y_true = K.cast(y_true, y_pred.dtype)\n",
        "    return K.square(y_pred - y_true)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F59Mw8vSrl39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data, save_dir):\n",
        "  (x_train, y_train), (x_test, y_test) = data\n",
        "  \n",
        "  log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
        "  tb = callbacks.TensorBoard(log_dir = save_dir + '/tensorboard-logs', batch_size = batch_size, histogram_freq = int(debug))\n",
        "  checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5', monitor = 'val_capsnet_acc', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "  #lr_decay = callbacks.LearningRateScheduler(schedule = lambda epoch : args.lr * (args.lr_decay ** epoch))\n",
        "  \n",
        "  model.compile(optimizer = optimizers.Adam(), loss = [margin_loss, sum_squared_error], loss_weights = [1., 0.0005], metrics = {'capsnet': 'accuracy'})\n",
        "  \n",
        "  model.fit([x_train, y_train], [y_train, x_train], batch_size = batch_size, epochs = epochs, validation_data = [[x_test, y_test], [y_test, x_test]], callbacks = [log, tb, checkpoint])\n",
        "  \n",
        "  model.fit([x_train, y_train], [y_train, x_train], batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint])\n",
        "\n",
        "  model.save_weights(save_dir + '/trained_model.h5')\n",
        "  print('Trained model saved to \\ %s/trained_model.h5\\'' %  (save_dir))\n",
        "  #plot_log(save_dir + '/log.csv', show = True)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX1z1af_7YWY",
        "colab_type": "code",
        "outputId": "09d0daf4-c4d3-4869-aed5-b62ebfccb2b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def test(model, data, save_dir):\n",
        "    x_test, y_test = data\n",
        "    y_pred, x_recon = model.predict(x_test, batch_size=100)\n",
        "    print('-'*30 + 'Begin: test' + '-'*30)\n",
        "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
        "'''\n",
        "    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
        "    image = img * 255\n",
        "    Image.fromarray(image.astype(np.uint8)).save(save_dir + \"/real_and_recon.png\")\n",
        "    print()\n",
        "    print('Reconstructed images are saved to %s/real_and_recon.png' % save_dir)\n",
        "    print('-' * 30 + 'End: test' + '-' * 30)\n",
        "    plt.imshow(plt.imread(save_dir + \"/real_and_recon.png\"))\n",
        "    plt.show()'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\\n    image = img * 255\\n    Image.fromarray(image.astype(np.uint8)).save(save_dir + \"/real_and_recon.png\")\\n    print()\\n    print(\\'Reconstructed images are saved to %s/real_and_recon.png\\' % save_dir)\\n    print(\\'-\\' * 30 + \\'End: test\\' + \\'-\\' * 30)\\n    plt.imshow(plt.imread(save_dir + \"/real_and_recon.png\"))\\n    plt.show()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW36v3UJ3D67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_mnist():\n",
        "    # the data, shuffled and split between train and test sets\n",
        "    from keras.datasets import mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
        "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
        "    y_train = to_categorical(y_train.astype('float32'))\n",
        "    y_test = to_categorical(y_test.astype('float32'))\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMUtPjBM1AdY",
        "colab_type": "code",
        "outputId": "8035acfb-dc6d-4c06-d09d-b29c42e34234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 100\n",
        "routings = 3\n",
        "shift_fraction = 0.1\n",
        "testing = False\n",
        "debug = True\n",
        "save_dir = './result'\n",
        "weights = None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import os\n",
        "    import argparse\n",
        "    from keras.preprocessing.image import ImageDataGenerator\n",
        "    from keras import callbacks\n",
        "\n",
        "    # setting the hyper parameters\n",
        "    #parser = argparse.ArgumentParser(description=\"Capsule Network on MNIST.\")\n",
        "    #parser.add_argument('--digit', default=5, type=int,\n",
        "    #                    help=\"Digit to manipulate\")\n",
        "\n",
        "    #args = parser.parse_args()\n",
        "    #print(args)\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # load data\n",
        "    (x_train, y_train), (x_test, y_test) = load_mnist()\n",
        "\n",
        "    # define model\n",
        "    model, eval_model = CapsNet(input_shape=x_train.shape[1:],\n",
        "                                                  n_class=len(np.unique(np.argmax(y_train, 1))),\n",
        "                                                  routings=routings)\n",
        "    model.summary()\n",
        "\n",
        "    # train or test\n",
        "    if weights is not None:  # init the model weights with provided one\n",
        "        model.load_weights(weights)\n",
        "    if not testing:\n",
        "        train(model=model, data=((x_train, y_train), (x_test, y_test)), save_dir=save_dir)\n",
        "    else:  # as long as weights are given, will run testing\n",
        "        if weights is None:\n",
        "            print('No weights are provided. Will test using random initialized weights.')\n",
        "        #manipulate_latent(manipulate_model, (x_test, y_test), args)\n",
        "        test(model=eval_model, data=(x_test, y_test), save_dir=save_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w shape:  (10, 1152, 16, 8)\n",
            "inputs shape:  (None, 1152, 8)\n",
            "inputs_expand shape:  (None, 1, None, 8)\n",
            "inputs_tiled shape:  (None, 10, None, 8)\n",
            "inputs_hat shape:  (None, 10, None, 1152, 16)\n",
            "b (weights) shape:  (None, 10, 1152)\n",
            "outputs shape:  (None, 10, 10, 1152, 16)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-479e1221bf8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     model, eval_model = CapsNet(input_shape=x_train.shape[1:],\n\u001b[1;32m     32\u001b[0m                                                   \u001b[0mn_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                                   routings=routings)\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a07cc0eca4ce>\u001b[0m in \u001b[0;36mCapsNet\u001b[0;34m(input_shape, n_class, routings)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m#Layer 3: Capsule Layer, Routing here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mdigit_caps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCapsuleLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_capsule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_capsule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroutings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroutings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'digitcaps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_caps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-4cfc5b1fe9a8>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (None, 10, 10, 1152, 16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;31m#raw_weights += tf.matmul(inputs_hat, outputs_tiled, transpose_a=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_dot\u001b[0;34m(x, y, axes)\u001b[0m\n\u001b[1;32m   1259\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' and '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                          \u001b[0;34m' with axes='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. x.shape[%d] != '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m                          'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2))\n\u001b[0m\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[0;31m# backup ndims. Need them later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can not do batch_dot on inputs with shapes (None, 10, 10, 1152, 16) and (None, 10, None, 1152, 16) with axes=[2, 3]. x.shape[2] != y.shape[3] (10 != 1152)."
          ]
        }
      ]
    }
  ]
}